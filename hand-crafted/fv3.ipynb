{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating SIFT descriptors. Number of images in ../dataset/food2/AngKuKueh is 200\n",
      "Calculating SIFT descriptors. Number of images in ../dataset/food2/AisKacang is 200\n",
      "Feature extration: 126.742870808 seconds\n",
      "Number of words  (128, 574883)\n",
      "('Training GMM of size', 64)\n",
      "GMM: 49.0598399639 seconds\n",
      "(128, 64)\n",
      "(128, 64)\n",
      "(64, 1)\n",
      "Encoding FV\n",
      "Fisher Vector: 199.400973082 seconds\n",
      "Training SVM\n",
      "(400,)\n",
      "SVM: 0.728898048401 seconds\n",
      "Score for fold  1 =  0.99\n",
      "SVM: 0.72122502327 seconds\n",
      "Score for fold  2 =  0.98\n",
      "Accuracy :  0.985\n",
      "Total: 382.75841713 seconds\n"
     ]
    }
   ],
   "source": [
    "#Author: Jacob Gildenblat, 2014\n",
    "#License: you may use this for whatever you like \n",
    "import sys, glob, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math, cv2\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import hickle as hkl\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from _vlfeat import *\n",
    "\n",
    "'''\n",
    "def dictionary(descriptors, N):\n",
    "    em = cv2.EM(N)\n",
    "    em.train(descriptors)\n",
    "\n",
    "    return np.float32(em.getMat(\"means\")), \\\n",
    "        np.float32(em.getMatVector(\"covs\")), np.float32(em.getMat(\"weights\"))[0]\n",
    "'''\n",
    "\n",
    "def dictionary(descriptors, N):\n",
    "    means, covs, priors, _ = vl_gmm(descriptors, N)\n",
    "    #save(\"means.gmm\", gmm.means_)\n",
    "    #save(\"covs.gmm\", gmm.covars_)\n",
    "    #save(\"weights.gmm\", gmm.weights_)\n",
    "    return means, covs, priors \n",
    "\n",
    "def image_descriptors(file):\n",
    "    img = cv2.imread(file, 0)\n",
    "    #img = cv2.resize(img, (256, 256))\n",
    "    img = np.array(img, 'f', order='F') # 'F' = column-major order!\n",
    "    img = np.array(img, 'float32')\n",
    "\n",
    "    f, descriptors = vl_sift(img,floatDescriptors=True, verbose=False) #0.7225 acc\n",
    "    #descriptor = cv2.DescriptorExtractor_create(\"OpponentSURF\")\n",
    "    #_ , descriptors = cv2.SIFT().detectAndCompute(img, None)\n",
    "    #descriptors = apply_pca(descriptors)\n",
    "    #f, descriptors = vl_dsift(img, fast=False, norm=True, step=100, floatDescriptors=True, verbose=False, size=5) #0.33 acc\n",
    "    #f, descriptors = vl_phow(img, verbose=False) #0.73125 128x128\n",
    "    descriptors = np.swapaxes(descriptors,0,1)\n",
    "    return descriptors\n",
    "\n",
    "def folder_descriptors(folder):\n",
    "    files = glob.glob(folder + \"/*.jpg\")\n",
    "    print \"Calculating SIFT descriptors. Number of images in \"+ folder +\" is \" + str(len(files))\n",
    "    return np.concatenate([image_descriptors(file) for file in files])\n",
    "\n",
    "def fisher_vector(samples, means, covs, w):\n",
    "    samples = np.swapaxes(samples,0,1)\n",
    "    fv = vl_fisher(samples, means, covs, w, fast=True, improved=True)\n",
    "    test = []\n",
    "    \n",
    "    for i in fv:\n",
    "        test = np.append(test , i[0])\n",
    "        \n",
    "    return test\n",
    "\n",
    "def apply_pca(image_descriptors):\n",
    "    pca = PCA(n_components=64)\n",
    "    return (pca.fit_transform(image_descriptors))\n",
    "\n",
    "def generate_gmm(input_folder, N):\n",
    "    loadfeature = False\n",
    "    \n",
    "    # start count execution time\n",
    "    start_time = time.time()\n",
    "    words = load_feature() if loadfeature else np.concatenate([folder_descriptors(folder) for folder in glob.glob(input_folder + '/*')]) \n",
    "    print(\"Feature extration: %s seconds\" % (time.time() - start_time))\n",
    "    \n",
    "    hkl.dump(words, 'pca_feature.hkl', mode='w')\n",
    "        \n",
    "    words = np.swapaxes(words,0,1)\n",
    "    print \"Number of words \", words.shape\n",
    "    print(\"Training GMM of size\", N)\n",
    "    start_time = time.time()\n",
    "    means, covs, weights = dictionary(words, N)\n",
    "    print(\"GMM: %s seconds\" % (time.time() - start_time))\n",
    "    \n",
    "    print means.shape\n",
    "    print covs.shape\n",
    "    print weights.shape\n",
    "\n",
    "    np.save(\"means.gmm\", means)\n",
    "    np.save(\"covs.gmm\", covs)\n",
    "    np.save(\"weights.gmm\", weights)\n",
    "    return means, covs, weights\n",
    "\n",
    "def get_fisher_vectors_from_folder(folder, gmm):\n",
    "    files = glob.glob(folder + \"/*.jpg\")\n",
    "    return np.float32([fisher_vector(image_descriptors(file), *gmm) for file in files])\n",
    "\n",
    "def fisher_features(folder, gmm):\n",
    "    print \"Encoding FV\"\n",
    "    folders = glob.glob(folder + \"/*\")\n",
    "    start_time = time.time()\n",
    "    features = {f : get_fisher_vectors_from_folder(f, gmm) for f in folders}\n",
    "    print(\"Fisher Vector: %s seconds\" % (time.time() - start_time))\n",
    "    with open( 'fv.pkl', 'wb') as f:\n",
    "        pickle.dump(features, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    hkl.dump(features, \"fv.hkl\", mode='w')\n",
    "    \n",
    "    return features\n",
    "\n",
    "def train(gmm, features):\n",
    "    print \"Training SVM\"\n",
    "    \n",
    "    X = np.concatenate(features.values())\n",
    "    y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
    "    print y.shape\n",
    "    \n",
    "    k = 2\n",
    "    sfold = StratifiedKFold(y, n_folds=k)\n",
    "    \n",
    "    total_score = 0\n",
    "    fold_count = 1\n",
    "\n",
    "    for train_index, test_index in sfold:\n",
    "        train_data, test_data = X[train_index], X[test_index]\n",
    "        train_label, test_label = y[train_index], y[test_index]\n",
    "        start_time = time.time()\n",
    "        clf = svm.SVC(kernel='rbf', gamma=0.7, C=1.0)\n",
    "        clf.fit(train_data, train_label)\n",
    "        print(\"SVM: %s seconds\" % (time.time() - start_time))\n",
    "        y_pred = clf.predict(test_data)\n",
    "    \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(test_label, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        #print(cm)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm)\n",
    "\n",
    "        # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "        # in each class)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print('Normalized confusion matrix')\n",
    "        #print(cm_normalized)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        score = clf.score(test_data,test_label)\n",
    "        print \"Score for fold \", fold_count, \"= \", score\n",
    "        total_score = total_score + score\n",
    "        fold_count = fold_count + 1\n",
    "\n",
    "    print \"Accuracy : \", total_score/k\n",
    "    return clf\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(100)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "    \n",
    "def load_gmm(folder = \"\"):  \n",
    "    f = file(\"means.gmm.npy\",\"rb\")\n",
    "    means = np.load(f)\n",
    "    \n",
    "    f = file(\"covs.gmm.npy\",\"rb\")\n",
    "    covs = np.load(f)\n",
    "    \n",
    "    f = file(\"weights.gmm.npy\",\"rb\") \n",
    "    weights = np.load(f)\n",
    "    \n",
    "    return means, covs, weights\n",
    "\n",
    "def load_fv():\n",
    "    with open('fv.pkl', 'rb') as f:\n",
    "        fv = pickle.load(f)\n",
    "    return fv\n",
    "\n",
    "def load_feature():\n",
    "    print \"Loading Feature\"\n",
    "    feature = hkl.load('pca_feature.hkl')\n",
    "    return feature\n",
    "\n",
    "number = 64\n",
    "working_folder = \"../dataset/food100\"\n",
    "gengmm_folder = \"../dataset/food100\"\n",
    "loadgmm = False\n",
    "loadfv = False\n",
    "\n",
    "total_time = time.time()\n",
    "  \n",
    "gmm = load_gmm(gengmm_folder) if loadgmm else generate_gmm(gengmm_folder, number)\n",
    "fisher_features = load_fv() if loadfv else fisher_features(working_folder, gmm)\n",
    "classifier = train(gmm, fisher_features)\n",
    "\n",
    "print(\"Total: %s seconds\" % (time.time() - total_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fisher_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-09ebcc138536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfisher_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfisher_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfisher_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fisher_features' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.concatenate(fisher_features.values())\n",
    "y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(fisher_features)), fisher_features.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    #f = open(\"../dataset/myfood10.pkl\", 'rb')\n",
    "\n",
    "    #d = pickle.load(f)\n",
    "    #data = d['trainFeatures']\n",
    "    #labels = d['trainLabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
