{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GMM..\n",
      "Loading SIFT fisher vector..\n",
      "Training SVM\n",
      "(20000,)\n",
      "SVM: 7364.92819691 seconds\n",
      "Acc for fold  1 =  0.4063\n",
      "[53 51 18 18 17 20 27 82 28 70 19 41 47 24 16 51 42 19 31 15 35 28 36 42 44\n",
      " 67 33 46 43 14 52 42 39 63 20 78 21 23 36 48 19 29 68 45 14 50 65 59 19 65\n",
      " 43 44 55 56 16 54 52 43 70 38 43 67 65 30 71 14 46 72 72 39 65 43 48 54 35\n",
      " 34 47 26 32 45 31 33 20 26 14 50 57 62 24 15 25 69 32 18 49 30 44 28 51 30]"
     ]
    }
   ],
   "source": [
    "#Author: Jacob Gildenblat, 2014\n",
    "#License: you may use this for whatever you like \n",
    "import sys, glob, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math, cv2\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import hickle as hkl\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from _vlfeat import *\n",
    "'''\n",
    "def dictionary(descriptors, N):\n",
    "    em = cv2.EM(N)\n",
    "    em.train(descriptors)\n",
    "\n",
    "    return np.float32(em.getMat(\"means\")), \\\n",
    "        np.float32(em.getMatVector(\"covs\")), np.float32(em.getMat(\"weights\"))[0]\n",
    "'''\n",
    "\n",
    "def dictionary(descriptors, N):\n",
    "    means, covs, priors, _ = vl_gmm(descriptors, N)\n",
    "    #save(\"means.gmm\", gmm.means_)\n",
    "    #save(\"covs.gmm\", gmm.covars_)\n",
    "    #save(\"weights.gmm\", gmm.weights_)\n",
    "    return means, covs, priors \n",
    "\n",
    "def image_descriptors(file):\n",
    "    img = cv2.imread(file, 0)\n",
    "    #img = cv2.resize(img, (256, 256))\n",
    "    img = np.array(img, 'f', order='F') # 'F' = column-major order!\n",
    "    img = np.array(img, 'float32')\n",
    "\n",
    "    f, descriptors = vl_sift(img,floatDescriptors=True, verbose=False) #0.7225 acc\n",
    "    #descriptor = cv2.DescriptorExtractor_create(\"OpponentSURF\")\n",
    "    #_ , descriptors = cv2.SIFT().detectAndCompute(img, None)\n",
    "    #descriptors = apply_pca(descriptors)\n",
    "    #f, descriptors = vl_dsift(img, fast=False, norm=True, step=100, floatDescriptors=True, verbose=False, size=5) #0.33 acc\n",
    "    #f, descriptors = vl_phow(img, verbose=False) #0.73125 128x128\n",
    "    descriptors = np.swapaxes(descriptors,0,1)\n",
    "    return descriptors\n",
    "\n",
    "def folder_descriptors(folder):\n",
    "    files = glob.glob(folder + \"/*.jpg\")\n",
    "    print \"Calculating SIFT descriptors. Number of images in \"+ folder +\" is \" + str(len(files))\n",
    "    return np.concatenate([image_descriptors(file) for file in files])\n",
    "\n",
    "def fisher_vector(samples, means, covs, w):\n",
    "    samples = np.swapaxes(samples,0,1)\n",
    "    fv = vl_fisher(samples, means, covs, w, fast=True, improved=True)\n",
    "    test = []\n",
    "    \n",
    "    for i in fv:\n",
    "        test = np.append(test , i[0])\n",
    "        \n",
    "    return test\n",
    "\n",
    "def apply_pca(image_descriptors):\n",
    "    pca = PCA(n_components=64)\n",
    "    return (pca.fit_transform(image_descriptors))\n",
    "\n",
    "def generate_gmm(input_folder, N):\n",
    "    loadfeature = False\n",
    "    \n",
    "    # start count execution time\n",
    "    start_time = time.time()\n",
    "    words = load_feature() if loadfeature else np.concatenate([folder_descriptors(folder) for folder in glob.glob(input_folder + '/*')]) \n",
    "    print(\"Feature extration: %s seconds\" % (time.time() - start_time))\n",
    "    hkl.dump(words, 'sift_feature.h5', mode='w')\n",
    "        \n",
    "    words = np.swapaxes(words,0,1)\n",
    "    print \"Number of words \", words.shape\n",
    "    print(\"Training GMM of size\", N)\n",
    "    start_time = time.time()\n",
    "    means, covs, weights = dictionary(words, N)\n",
    "    print(\"GMM: %s seconds\" % (time.time() - start_time))\n",
    "    \n",
    "    print means.shape\n",
    "    print covs.shape\n",
    "    print weights.shape\n",
    "\n",
    "    np.save(\"means.gmm\", means)\n",
    "    np.save(\"covs.gmm\", covs)\n",
    "    np.save(\"weights.gmm\", weights)\n",
    "    return means, covs, weights\n",
    "\n",
    "def get_fisher_vectors_from_folder(folder, gmm):\n",
    "    files = glob.glob(folder + \"/*.jpg\")\n",
    "    return np.float32([fisher_vector(image_descriptors(file), *gmm) for file in files])\n",
    "\n",
    "def fisher_features(folder, gmm):\n",
    "    print \"Encoding FV\"\n",
    "    folders = glob.glob(folder + \"/*\")\n",
    "    start_time = time.time()\n",
    "    features = {f : get_fisher_vectors_from_folder(f, gmm) for f in folders}\n",
    "    print(\"Fisher Vector: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "    f = open('sift_fv.pkl', 'wb')\n",
    "    pickle.dump(features, f)\n",
    "    #hkl.dump(features, \"sift_fv.h5\", mode='w')\n",
    "    \n",
    "    return features\n",
    "\n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        plt.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom')\n",
    "        \n",
    "def train(gmm, features):\n",
    "    X = np.concatenate(features.values())\n",
    "    y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
    "    print y.shape\n",
    "    \n",
    "    k = 2\n",
    "    sfold = StratifiedKFold(y, n_folds=k)\n",
    "    \n",
    "    total_score = 0\n",
    "    fold_count = 1\n",
    "\n",
    "    for train_index, test_index in sfold:\n",
    "        print \"Training SVM..\"\n",
    "        train_data, test_data = X[train_index], X[test_index]\n",
    "        train_label, test_label = y[train_index], y[test_index]\n",
    "        start_time = time.time()\n",
    "        clf = svm.SVC(kernel='linear', C=1.0, probability=True)\n",
    "        clf.fit(train_data, train_label)\n",
    "        print(\"SVM: %s seconds\" % (time.time() - start_time))\n",
    "        \n",
    "        #print \"Saving SIFT SVM model..\"\n",
    "        #hkl.dump(clf, \"sift_svm_\"+ str(fold_count) +\".h5\")\n",
    "        \n",
    "        y_pred = clf.predict(test_data)\n",
    "    \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(test_label, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        #print(cm)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm)\n",
    "\n",
    "        # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "        # in each class)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print('Normalized confusion matrix')\n",
    "        #print(cm_normalized)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        score = clf.score(test_data,test_label)\n",
    "        print \"Acc for fold \", fold_count, \"= \", score\n",
    "        \n",
    "        scores = clf.predict_proba(test_data)\n",
    "        n = 5\n",
    "        indices = np.argsort(scores)[:,:-n-1:-1]\n",
    "\n",
    "        # Get accuracy\n",
    "        top1 = 0.0\n",
    "        top5 = 0.0\n",
    " \n",
    "        correct_predict_top1 = np.zeros((100,), dtype=np.int)\n",
    "        correct_predict_top5 = np.zeros((100,), dtype=np.int)\n",
    "        \n",
    "        for image_index, index_list in enumerate(indices):\n",
    "            if test_label[image_index] == index_list[0]:\n",
    "                top1 += 1.0\n",
    "            if test_label[image_index] in index_list:\n",
    "                top5 += 1.0\n",
    "        \n",
    "        image_index = None    \n",
    "        index_list = None\n",
    "        start_index = 0\n",
    "        end_index = 99\n",
    "\n",
    "        for class_label in range(0,100):\n",
    "            for image_index in range(start_index,end_index+1):\n",
    "                if test_label[image_index] == indices[image_index][0]:\n",
    "                    correct_predict_top1[class_label] += 1\n",
    "                if test_label[image_index] in indices[image_index]:\n",
    "                    correct_predict_top5[class_label] += 1\n",
    "            start_index += 100\n",
    "            end_index += 100\n",
    "\n",
    "        objects = ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    "         'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    "         'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    "         'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    "         'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    "         'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    "         'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    "         'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    "         'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    "         'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    "         'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    "         'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    "         'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    "         'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    "         'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    "         'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    "         'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    "         'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng']\n",
    "        y_pos = np.arange(len(objects))\n",
    "        performance = correct_predict_top1\n",
    "\n",
    "        rects1 = plt.bar(y_pos, performance)\n",
    "        plt.xticks(y_pos, objects, rotation='vertical')\n",
    "        plt.ylabel('Total true positive')\n",
    "        plt.title('Total true positive per sample')\n",
    "\n",
    "        autolabel(rects1)\n",
    "        plt.savefig('barchart_deep_feaures'+ str(fold_count) +'.png')\n",
    "        plt.show()\n",
    "\n",
    "        print correct_predict_top1\n",
    "        print correct_predict_top5\n",
    "\n",
    "        print('Top-1 Accuracy: ' + str(top1 / len(test_label) * 100.0) + '%')\n",
    "        print('Top-5 Accuracy: ' + str(top5 / len(test_label) * 100.0) + '%')\n",
    "        total_score = total_score + top1 / len(test_label)\n",
    "        fold_count = fold_count + 1\n",
    "\n",
    "    print \"Accuracy : \", total_score/k\n",
    "    return clf\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(100)\n",
    "    plt.xticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['AisKacang' , 'AngKuKueh' , 'ApamBalik' , 'Asamlaksa' , 'Bahulu' , 'Bakkukteh',\n",
    " 'BananaLeafRice' , 'Bazhang' , 'BeefRendang' , 'BingkaUbi' , 'Buburchacha',\n",
    " 'Buburpedas' , 'Capati' , 'Cendol' , 'ChaiTowKuay' , 'CharKuehTiao' , 'CharSiu',\n",
    " 'CheeCheongFun' , 'ChiliCrab' , 'Chweekueh' , 'ClayPotRice' , 'CucurUdang',\n",
    " 'CurryLaksa' , 'CurryPuff' , 'Dodol' , 'Durian' , 'DurianCrepe' , 'FishHeadCurry',\n",
    " 'Guava' , 'HainaneseChickenRice' , 'HokkienMee' , 'Huatkuih' , 'IkanBakar',\n",
    " 'Kangkung' , 'KayaToast' , 'Keklapis' , 'Ketupat' , 'KuihDadar' , 'KuihLapis',\n",
    " 'KuihSeriMuka' , 'Langsat' , 'Lekor' , 'Lemang' , 'LepatPisang' , 'LorMee',\n",
    " 'Maggi goreng' , 'Mangosteen' , 'MeeGoreng' , 'MeeHoonKueh' , 'MeeHoonSoup',\n",
    " 'MeeJawa' , 'MeeRebus' , 'MeeRojak' , 'MeeSiam' , 'Murtabak' , 'Murukku',\n",
    " 'NasiGorengKampung' , 'NasiImpit' , 'Nasikandar' , 'Nasilemak' , 'Nasipattaya',\n",
    " 'Ondehondeh' , 'Otakotak' , 'OysterOmelette' , 'PanMee' , 'PineappleTart',\n",
    " 'PisangGoreng' , 'Popiah' , 'PrawnMee' , 'Prawnsambal' , 'Puri' , 'PutuMayam',\n",
    " 'PutuPiring' , 'Rambutan' , 'Rojak' , 'RotiCanai' , 'RotiJala' , 'RotiJohn',\n",
    " 'RotiNaan' , 'RotiTissue' , 'SambalPetai' , 'SambalUdang' , 'Satay' , 'Sataycelup',\n",
    " 'SeriMuka' , 'SotoAyam' , 'TandooriChicken' , 'TangYuan' , 'TauFooFah',\n",
    " 'TauhuSumbat' , 'Thosai' , 'TomYumSoup' , 'Wajik' , 'WanTanMee' , 'WaTanHo' , 'Wonton',\n",
    " 'YamCake' , 'YongTauFu' , 'Youtiao' , 'Yusheng'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "    \n",
    "def load_gmm(folder = \"\"): \n",
    "    print \"Loading GMM..\"\n",
    "    f = file(\"means.gmm.npy\",\"rb\")\n",
    "    means = np.load(f)\n",
    "    \n",
    "    f = file(\"covs.gmm.npy\",\"rb\")\n",
    "    covs = np.load(f)\n",
    "    \n",
    "    f = file(\"weights.gmm.npy\",\"rb\") \n",
    "    weights = np.load(f)\n",
    "    \n",
    "    return means, covs, weights\n",
    "\n",
    "def load_fv():\n",
    "    print \"Loading SIFT fisher vector..\"\n",
    "    with open('sift_fv.pkl', 'rb') as f:\n",
    "        fv = pickle.load(f)\n",
    "    #fv = hkl.load(\"sift_fv.h5\")\n",
    "    return fv\n",
    "\n",
    "def load_feature():\n",
    "    print \"Loading SIFT Feature..\"\n",
    "    with open('sift_feature.pkl', 'rb') as f:\n",
    "        feature = pickle.load(f)\n",
    "    #feature = hkl.load('sift_feature.pkl')\n",
    "    return feature\n",
    "\n",
    "number = 32\n",
    "working_folder = \"../dataset/food100\"\n",
    "gengmm_folder = \"../dataset/food100\"\n",
    "loadgmm = False\n",
    "loadfv = False\n",
    "\n",
    "total_time = time.time()\n",
    "  \n",
    "gmm = load_gmm(gengmm_folder) if loadgmm else generate_gmm(gengmm_folder, number)\n",
    "fisher_features = load_fv() if loadfv else fisher_features(working_folder, gmm)\n",
    "classifier = train(gmm, fisher_features)\n",
    "\n",
    "print(\"Total: %s seconds\" % (time.time() - total_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    #f = open(\"../dataset/myfood10.pkl\", 'rb')\n",
    "\n",
    "    #d = pickle.load(f)\n",
    "    #data = d['trainFeatures']\n",
    "    #labels = d['trainLabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
